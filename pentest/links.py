import re
import requests
from typing import List, Set, Dict
from urllib.parse import urljoin, urlparse


def get_links(text: str) -> List[str]:
    links = re.findall("""href="[^"]*""", text)
    clean_links = list()
    for link in links:
        clean_link = link[6:]
        clean_links.append(clean_link)
    return clean_links


def get_links_from_url(url: str) -> List[str]:
    r = requests.get(url)
    links = get_links(r.text)
    return links


def cleanup_links(links: List[str], original_url: str) -> Set[str]:
    links = (link for link in links if not link.startswith("#"))
    links = (urljoin(original_url, link) for link in links)
    orig_netloc = urlparse(original_url).netloc
    links = (link for link in links if urlparse(link).netloc == orig_netloc)
    return set(links)


def get_all_pages(start_url: str) -> Dict[str, str]:
    pages = dict()
    r = requests.get(start_url)
    pages[start_url] = r.text
    urls = get_links(r.text)
    urls = cleanup_links(urls, start_url)

    for url in urls:
        r = requests.get(url)
        content_type = r.headers.get('content_type')
        if not content_type:
            continue
        if "text/html" not in content_type:
            continue
        pages[url] = r.text
    return pages
